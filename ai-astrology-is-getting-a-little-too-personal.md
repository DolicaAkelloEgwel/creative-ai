An article that touches upon #technomancy and AI divination. Discusses the use of AI in the popular Co-Star app for creating more personalised messages.

Link: https://www.theatlantic.com/technology/archive/2023/12/co-star-app-ask-stars-chatbot-ai-astrology/676274/

>The language is humanlike—it relies on models created by OpenAI, the same company behind ChatGPT—and the citation of both my astrological chart and NASA data lend the responses a peculiar authority. I can’t lie: They’re compelling.

NASA data?

>Still, the arrival of “Ask the stars” is a prism into the complex ways that new advances in generative AI can seep into people’s spiritual and moral lives, even through the most mundane decisions. 

Leading to "over-divination."

>But imagine an AI chatbot that’s trained on your own preferences and habits telling you that exercising in the morning will set you up for success. Things are murkier if that success never arrives. Maybe you just need to wait longer. Maybe the problem is you.

It's too natural-sounding to be wrong.

>Whenever people perceive AI as better, faster, and more efficient than humans, “our assumption of its superiority places it up in this godlike space,” Singler said. That assumption, she cautioned, “obscures all the humans in the machine.” AI chatbots summon clear, definite answers as if by magic, with little indication that the technology itself is made up of our own convictions, flaws, and biases fed into algorithms.

It's especially easy to forget that AI is flawed.

>Clear, definite answers have an obvious appeal, especially when the world feels unpredictable; over the first year of the pandemic, for instance, searches for birth chart and astrology reached a five-year high worldwide. In times of crisis, one has to wonder how willing some people might be to look to chatbots like Co–Star’s for guidance—to outsource decision making, however big or small.

COVID was also [helpful for psychics](https://spectrumlocalnews.com/tx/south-texas-el-paso/news/2021/03/07/psychics--tarot-card-readers-become-more-popular-during-pandemic).

>The bot resets after each question, no follow-ups allowed, to try to prevent people from falling too far down the rabbit hole.

>Co–Star further claims that the chatbot rejects 20 percent of questions because of “potential risks”—queries about self-harm, for example.

Co-Star are trying to prevent misuse.

>“The question isn’t how do we prevent dependency, which I think is a solvable but not terribly interesting question,” she continued, “**but more like how do we make every sentence hit? Like, really hit?**” I nodded while she took a pull from her vape.

The solution?