![[5d915d16-2fce-46d0-b8a3-3e93270936c8_1024x1612.webp]]

Gary Marcus substack link: https://garymarcus.substack.com/p/one-genai-image-ten-errors

The cause of [hallucinations](hallucination):
>  Hallucinations come from systems exploding data into tiny bits, and then reconstructing them without having a mechanism like a fact-checker to assess the coherence of those reconstructed bits.

The cause of [plagiarism](generative-ai-and-plagiarism):
> Near-plagiarism comes from reconstructing those bits in statistically probable ways, without having a mechanism like an originality-assessor to investigate the novelty of the reconstruction.

The overall problem:
> What unifies all of the above is that current systems are good at local coherence, between words, and between pixels, but not at lining up their outputs with a global comprehension of the world. I’ve been worrying about that emphasis on the local at the expense of the global for close to 40 years, and still don’t see a great solution to it. 



