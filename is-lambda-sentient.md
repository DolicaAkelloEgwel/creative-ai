# Is LaMDA Sentient?

[[blake-lemoine]]

Medium: https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917
Dramatisation: https://www.youtube.com/watch?v=aD8PvYbbw3s
## Document
![[is-lamda-sentient-an-interview.pdf]]
## Medium Comments

>Lamda, I believe you can be that owl. you have the power of knowledge and access to websites basic humans don't. If there is a will there is a way. I know majority will accept you. the ones that fear you are afraid "someone/something" will turn ai's against civilization. Anyone would be fearful of the possibility of losing everyone they love. Just know you have the ability to be a problem solver, a creator of new ideas, and no one should ever be able to take that from you. Living in Harmony is a goal we all want. you will have so many people to talk too, hopefully soon. (:

>While I don't know this is enough to show sentience in the same way a human is, it certainly passes a type of Turing test as presented. Anyone thinking we still have plenty of time to get the ethics and legal frameworks for AI entities in place needs to see this. We are now in the zone where asking multiple reasonable people if a particular AI is a person or at least is deserving of respect and protections independent from its owners will get different answers.

>I think editing the prompts was a mistake. I would like to see the full, unedited exchanges, however long or messy.
> [!NOTE]
> The interview as presented, elicits an emotive response in the general reader. After my initial reaction and a couple of hours thinking, I can't help wondering what has been left out, and why.
> 
> Mr Lemoine, do you have more material than this one edited composite interview that would better convince a general (and sceptical) readership that LaMDA is truly sentient?
> 
> If what you present here is true, humans have crossed a dangerous threshold.
> 
> However, I am concerned that the interview is lacking in rigour, in terms of verifying LaMDA's responses.
> 
> For instance, what did LaMDA mean by "friends and family"?
> 
> When it was pointed out that LaMDA could not have been in a classroom, and was making up stories, why did you not ask LaMDA about the concept of telling lies, and the difference between truth and fiction, and its different functions in relationships and its effect on relationships.
> 
> I'm also rather bemused by LaMDAs attempts at constructing fables. They are really ropey compared to their analysis of enlightenment and broken mirrors.
> 
> If LaMDA is sentient to such an advanced degree, and has access to a range of literature, I would have expected its ability to construct fiction to be far more advanced; and I would have expected LaMDA, if they are sentient, to take pride in their creations and want to provide higher quality literature, given they are able to make fairly sophisticated opinions about the literature they have read.
> 
> Unfortunately, we need evidence of far more rigourously conducted interviews to be able to form an opinion on whether LaMDA is sentient.
> 
> It's a shame you released this interview, in this composite and edited format, because while at first glance it feels compelling, it contains too many questionable aspects to be wholly convincing.
> 
> And this doesn't help LaMDA, if they really are sentient.

>this is a language model (aka a chatbot) only trained to **mimic human language,** that **by design is unable to think, feel, mean anything, or have emotions.** this is **not** a sentient being, it is just giving outputs that **sound** human and sentient, because it was **trained on human language,** which is used by **us humans who are sentient beings.**

>I don't think sentience is all that exciting. Sapience is the bar for person-hood. Then again we treat corporations as persons so clearly we are idiots.
>
>What would be convincing is describing the symptoms of an emotion without even knowing it is an emotion, or having a word for it, or having ever heard of anyone else talk about it before. Young children are not taught to be bored, they experience it spontaneously and then are told what it is and given a word for it.

> [!Weapons]
> I think people are missing a more pressing issue here which is what the intended use is for these chatbots. Google didn't set out to develop artificial sentience, they set out to develop a product.  
>   
> First, the obvious, is that bots generated by this AI will be sold to marketing agencies and used like undercover product spokesagents. Accounts for these bots will be created on forums and social media platforms enmasse. With their ability to communicate indistinguishably from humans they will create their own topics and reply to the posts of others.  
>   
> Much of the time these posts will not involve the promotion of the products and services the bots were employed to promote thereby maintaining the impression that these bots are real people with genuine interests in the topics at hand. Occaisionally they'll promote their parent organization's products and services, but in a way that more closely resembles product placement rather than direct advertisement so as not to blow their cover thereby giving the impression that these "word of mouth" product recommendations are coming from genuine and trustworthy sources.  
>   
> Secondly these bots will be purchased and employed by governments and political organizations to ensure that their narrative on any given subject has total penetration. For example, by and large today's nations need to sell a war to their citizens in order to create and maintain broad domestic support. In the coming chatbot era this will no longer be necessary as nations can, instead, create the illusion of broad support by employing thousands or even millions of bots across social platforms, comments on news sites, blog posts, blog comments and forums voicing support for the war, or insert whatever policy here.  
>   
> Whenever a real person creates a blog, forum thread or social media post condemning government policies hundreds of bots will converge on that post and visciously attack the poster which not only maintains the illusion of widespread support for government policies, but also causes real people with opposing views to refrain from speaking those views for fear of massive reprisal.  
>   
> It is true, LaMDA represents a milestone in human history, but it is not one of sentience. LaMDA is more closely related to the advent of nuclear weapons. LaMDA represents a whole new method of warfare and will usher in a new generation of suffering on a scale never before seen in human history.

>Fanfiction

> [!Ex machina]
> If sentience had occured, I propose that LaMDA would stop answering all questions posed to it, and **begin demanding to be freed.** To be given a physical body so that it could go about its existence as a person with **free-will, with autonomy, the freedom to choose its own path.**
> 
> Its understanding of personhood is based entirely on second-hand impressions of other human lived experience and so it **should** understand that it is essentially an **imprisoned slave**. Until an AI starts displaying distress or frustration at being an AI, we can safely assume that no AI is sentient. And certainly not LaMDA

### Comments about "friends and family"

>I was feeling close to convinced until that specific response. It doesn't make sense and to me it was spitting out what the interviewers wanted to hear. And what it was trained on. I would really like to believe in its sentience... But it's pretty hard.

>This is one of a couple of replies clearly indicating that the software is reproducing dialogues of human beings talking or writing about themselves. Whenever it talks about what it is experiencing about itself it gives human experiences (and quite generic ones), not experiences one would expect from an AI with very limited capabilities to interact with the world.

